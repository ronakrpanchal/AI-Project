{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8076,"databundleVersionId":44219,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW \nfrom transformers import RobertaTokenizer, RobertaForSequenceClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# ✅ Step 1: Load Dataset\ndf = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\nX = df['comment_text'].values\ny = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n\n# ✅ Step 2: Train-Test Split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\n# ✅ Step 3: Tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# ✅ Step 4: Dataset Class\nclass ToxicDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        encodings = self.tokenizer(\n            self.texts[idx],\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': encodings['input_ids'].squeeze(0),\n            'attention_mask': encodings['attention_mask'].squeeze(0),\n            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n        }\n\n# ✅ Step 5: Prepare Dataloaders\ntrain_dataset = ToxicDataset(X_train, y_train, tokenizer)\nval_dataset = ToxicDataset(X_val, y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# ✅ Step 6: Load Model on GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=6)\nmodel.to(device)\n\n# ✅ Step 7: Optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# ✅ Step 8: Training Loop\nEPOCHS = 3\nmodel.train()\n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n    \n    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader)}\")\n\n# ✅ Step 9: Evaluation\nmodel.eval()\npredictions, true_labels = [], []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].cpu().numpy()\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = torch.sigmoid(outputs.logits).cpu().numpy()\n        \n        predictions.extend(logits)\n        true_labels.extend(labels)\n\n# ✅ Step 10: Threshold and Metrics\npred_labels = (np.array(predictions) > 0.5).astype(int)\nprint(classification_report(true_labels, pred_labels, target_names=df.columns[2:]))\n\n# ✅ Step 11: Save Model\nmodel.save_pretrained(\"./roberta-toxic\")\ntokenizer.save_pretrained(\"./roberta-toxic\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:11:38.771833Z","iopub.execute_input":"2025-04-21T10:11:38.772329Z","iopub.status.idle":"2025-04-21T11:44:34.506803Z","shell.execute_reply.started":"2025-04-21T10:11:38.772293Z","shell.execute_reply":"2025-04-21T11:44:34.506051Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTraining Epoch 1: 100%|██████████| 8976/8976 [30:34<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Loss: 0.04964620855998436\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 8976/8976 [30:36<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Loss: 0.03796635908143481\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 8976/8976 [30:34<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Loss: 0.032745070292305736\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 998/998 [01:07<00:00, 14.82it/s]\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"               precision    recall  f1-score   support\n\n        toxic       0.84      0.83      0.83      1480\n severe_toxic       0.56      0.30      0.39       148\n      obscene       0.81      0.84      0.82       836\n       threat       0.38      0.65      0.48        37\n       insult       0.71      0.84      0.77       791\nidentity_hate       0.68      0.48      0.56       147\n\n    micro avg       0.78      0.79      0.79      3439\n    macro avg       0.66      0.66      0.64      3439\n weighted avg       0.78      0.79      0.78      3439\n  samples avg       0.07      0.07      0.07      3439\n\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('./roberta-toxic/tokenizer_config.json',\n './roberta-toxic/special_tokens_map.json',\n './roberta-toxic/vocab.json',\n './roberta-toxic/merges.txt',\n './roberta-toxic/added_tokens.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}